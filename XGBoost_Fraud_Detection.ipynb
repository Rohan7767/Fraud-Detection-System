{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e-ZzeAGtEoPD"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'xgboost'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moutliers_influence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variance_inflation_factor\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, roc_auc_score, precision_recall_curve, auc, average_precision_score)\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from xgboost import XGBClassifier\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "import shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DauQq49QEtM8",
        "outputId": "8146015d-56e4-4caa-d3be-3e93d55d40de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded. Shape: (6362620, 11)\n",
            "   step      type    amount     nameOrig  oldbalanceOrg  newbalanceOrig     nameDest  oldbalanceDest  newbalanceDest  isFraud  isFlaggedFraud\n",
            "0     1   PAYMENT   9839.64  C1231006815       170136.0       160296.36  M1979787155             0.0             0.0        0               0\n",
            "1     1   PAYMENT   1864.28  C1666544295        21249.0        19384.72  M2044282225             0.0             0.0        0               0\n",
            "2     1  TRANSFER    181.00  C1305486145          181.0            0.00   C553264065             0.0             0.0        1               0\n",
            "3     1  CASH_OUT    181.00   C840083671          181.0            0.00    C38997010         21182.0             0.0        1               0\n",
            "4     1   PAYMENT  11668.14  C2048537720        41554.0        29885.86  M1230701703             0.0             0.0        0               0\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = Path('/content/Fraud.csv')\n",
        "OUTPUT_DIR = Path('fraud_analysis_outputs')\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Dataset not found at {DATA_PATH}. Please place 'Fraud.csv' in the same folder.\")\n",
        "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
        "print('Data loaded. Shape:', df.shape)\n",
        "print(df.head().to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-TT0-GIVFEzt",
        "outputId": "f27a97c8-418f-4dce-94c6-cfc611f64811"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6362620 entries, 0 to 6362619\n",
            "Data columns (total 11 columns):\n",
            " #   Column          Dtype  \n",
            "---  ------          -----  \n",
            " 0   step            int64  \n",
            " 1   type            object \n",
            " 2   amount          float64\n",
            " 3   nameOrig        object \n",
            " 4   oldbalanceOrg   float64\n",
            " 5   newbalanceOrig  float64\n",
            " 6   nameDest        object \n",
            " 7   oldbalanceDest  float64\n",
            " 8   newbalanceDest  float64\n",
            " 9   isFraud         int64  \n",
            " 10  isFlaggedFraud  int64  \n",
            "dtypes: float64(5), int64(3), object(3)\n",
            "memory usage: 534.0+ MB\n",
            "None\n",
            "\n",
            "--- Missing values (count & %) ---\n",
            "                missing_count  missing_pct\n",
            "step                        0          0.0\n",
            "type                        0          0.0\n",
            "amount                      0          0.0\n",
            "nameOrig                    0          0.0\n",
            "oldbalanceOrg               0          0.0\n",
            "newbalanceOrig              0          0.0\n",
            "nameDest                    0          0.0\n",
            "oldbalanceDest              0          0.0\n",
            "newbalanceDest              0          0.0\n",
            "isFraud                     0          0.0\n",
            "isFlaggedFraud              0          0.0\n"
          ]
        }
      ],
      "source": [
        "print('\\n--- Data Info ---')\n",
        "print(df.info())\n",
        "print('\\n--- Missing values (count & %) ---')\n",
        "miss = df.isnull().sum()\n",
        "print(pd.concat([miss, (miss/len(df))*100], axis=1).rename(columns={0:'missing_count',1:'missing_pct'}).sort_values('missing_count', ascending=False).head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIDLE01eFGML",
        "outputId": "949bbea2-a984-4fb5-8409-681d14eb05c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Numeric columns: ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest', 'isFraud', 'isFlaggedFraud']\n",
            "Categorical columns: ['type', 'nameOrig', 'nameDest']\n",
            "\n",
            "After imputation missing values: 0\n"
          ]
        }
      ],
      "source": [
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "print('\\nNumeric columns:', num_cols)\n",
        "print('Categorical columns:', cat_cols)\n",
        "thresh = 0.8\n",
        "high_missing = [(c, df[c].isnull().mean()) for c in df.columns if df[c].isnull().mean() > thresh]\n",
        "if high_missing:\n",
        "    print('\\nDropping columns with >80% missing:')\n",
        "    for c, pct in high_missing:\n",
        "        print(c, pct)\n",
        "    df.drop(columns=[c for c,p in high_missing], inplace=True)\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
        "if cat_cols:\n",
        "    df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
        "print('\\nAfter imputation missing values:', df.isnull().sum().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn6tL6ifFqw_",
        "outputId": "eb61b0a1-f89e-4df1-ee69-d5323fc1e415"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Outliers capped using IQR method for numeric variables.\n"
          ]
        }
      ],
      "source": [
        "for c in num_cols:\n",
        "    if df[c].nunique() > 10:\n",
        "        Q1 = df[c].quantile(0.25)\n",
        "        Q3 = df[c].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower = Q1 - 1.5 * IQR\n",
        "        upper = Q3 + 1.5 * IQR\n",
        "        df[c] = np.where(df[c] < lower, lower, df[c])\n",
        "        df[c] = np.where(df[c] > upper, upper, df[c])\n",
        "print('\\nOutliers capped using IQR method for numeric variables.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9ikMNpSFy5O",
        "outputId": "02137de2-78da-45d9-8f94-6eaca88d189f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Possible ID-like columns to drop for modeling: []\n",
            "Found target column: isFraud\n",
            "\n",
            "VIF top features:\n",
            "       feature       VIF\n",
            "newbalanceDest 36.858666\n",
            "oldbalanceDest 33.025454\n",
            "newbalanceOrig  6.489256\n",
            " oldbalanceOrg  5.923001\n",
            "        amount  2.680130\n",
            "          step  1.758521\n",
            "isFlaggedFraud  1.000037\n",
            "\n",
            "Features with VIF > 10 (will be dropped): ['newbalanceDest', 'oldbalanceDest']\n"
          ]
        }
      ],
      "source": [
        "possible_id_cols = [c for c in df.columns if c.lower() in ('id','transactionid','transaction_id','cust_id','customer_id')]\n",
        "print('\\nPossible ID-like columns to drop for modeling:', possible_id_cols)\n",
        "df_model = df.copy()\n",
        "df_model.drop(columns=possible_id_cols, inplace=True, errors='ignore')\n",
        "num_for_vif = df_model.select_dtypes(include=[np.number]).copy()\n",
        "target_col_candidates = [c for c in df_model.columns if c.lower() in ('isfraud','fraud','label','target')]\n",
        "if target_col_candidates:\n",
        "    target_col = target_col_candidates[0]\n",
        "    print('Found target column:', target_col)\n",
        "    if target_col in num_for_vif.columns:\n",
        "        num_for_vif.drop(columns=[target_col], inplace=True)\n",
        "else:\n",
        "    target_col = None\n",
        "num_for_vif = num_for_vif.loc[:, num_for_vif.nunique() > 1]\n",
        "def compute_vif(dfv):\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data['feature'] = dfv.columns\n",
        "    vif_data['VIF'] = [variance_inflation_factor(dfv.values, i) for i in range(dfv.shape[1])]\n",
        "    return vif_data.sort_values('VIF', ascending=False)\n",
        "if not num_for_vif.empty:\n",
        "    vif_df = compute_vif(num_for_vif.fillna(0))\n",
        "    print('\\nVIF top features:')\n",
        "    print(vif_df.head(20).to_string(index=False))\n",
        "    high_vif = vif_df[vif_df['VIF'] > 10]['feature'].tolist()\n",
        "    print('\\nFeatures with VIF > 10 (will be dropped):', high_vif)\n",
        "    df_model.drop(columns=high_vif, inplace=True, errors='ignore')\n",
        "else:\n",
        "    print('No numeric features for VIF check.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-9Rj4yYF8q9",
        "outputId": "96b5a44f-a2b9-4578-fed6-75f4208dfdce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Using target column: isFraud\n",
            "Low-cardinality categoricals (one-hot): ['type']\n",
            "High-cardinality categoricals (dropping): ['nameOrig', 'nameDest']\n"
          ]
        }
      ],
      "source": [
        "if target_col is None:\n",
        "    for c in df_model.columns:\n",
        "        if 'fraud' in c.lower() and df_model[c].nunique() <= 2:\n",
        "            target_col = c\n",
        "            break\n",
        "if target_col is None:\n",
        "    raise ValueError('Target column not identified automatically. Please set `target_col` variable manually.')\n",
        "print('\\nUsing target column:', target_col)\n",
        "X = df_model.drop(columns=[target_col])\n",
        "y = df_model[target_col].astype(int)\n",
        "cat_cols = X.select_dtypes(include=['object','category']).columns.tolist()\n",
        "low_card_cats = [c for c in cat_cols if X[c].nunique() <= 20]\n",
        "high_card_cats = [c for c in cat_cols if X[c].nunique() > 20]\n",
        "print('Low-cardinality categoricals (one-hot):', low_card_cats)\n",
        "print('High-cardinality categoricals (dropping):', high_card_cats)\n",
        "X.drop(columns=high_card_cats, inplace=True)\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', StandardScaler(), X.select_dtypes(include=[np.number]).columns.tolist()),\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), low_card_cats)\n",
        "], remainder='drop')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLsyz9DfGBo3",
        "outputId": "247d5f64-5af3-4c8c-df52-f58395d1b89d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Train shape: (5090096, 6) Test shape: (1272524, 6)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "print('\\nTrain shape:', X_train.shape, 'Test shape:', X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHdhzUnVGL_1",
        "outputId": "59dfadbb-2966-4a94-aa87-623f2152a46d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Running grid search over 8 models (this may take a while)\n",
            "params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100} AP: 0.4691\n",
            "params: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200} AP: 0.5842\n",
            "params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 100} AP: 0.6944\n",
            "params: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200} AP: 0.7919\n",
            "params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100} AP: 0.1082\n",
            "params: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200} AP: 0.296\n",
            "params: {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 100} AP: 0.3782\n",
            "params: {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 200} AP: 0.3885\n",
            "\n",
            "Best params by AP: {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 200} Best AP: 0.7918747510654136\n"
          ]
        }
      ],
      "source": [
        "pipeline = Pipeline(steps=[('pre', preprocessor), ('model', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42))])\n",
        "param_grid = {\n",
        "    'model__n_estimators': [100, 200],\n",
        "    'model__max_depth': [3, 6],\n",
        "    'model__learning_rate': [0.1, 0.01]\n",
        "}\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
        "class SMOTE_XGB(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, estimator=None):\n",
        "        self.estimator = estimator if estimator is not None else XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "    def fit(self, X, y):\n",
        "        sm = SMOTE(random_state=42)\n",
        "        Xs, ys = sm.fit_resample(X, y)\n",
        "        self.estimator_ = clone(self.estimator)\n",
        "        self.estimator_.fit(Xs, ys)\n",
        "        return self\n",
        "    def predict(self, X):\n",
        "        return self.estimator_.predict(X)\n",
        "    def predict_proba(self, X):\n",
        "        return self.estimator_.predict_proba(X)\n",
        "X_train_trans = preprocessor.fit_transform(X_train)\n",
        "X_test_trans = preprocessor.transform(X_test)\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "grid = list(ParameterGrid({\n",
        "    'n_estimators':[100,200], 'max_depth':[3,6],'learning_rate':[0.1,0.01]\n",
        "}))\n",
        "print('\\nRunning grid search over', len(grid), 'models (this may take a while)')\n",
        "best_score = -np.inf\n",
        "best_params = None\n",
        "best_clf = None\n",
        "for params in grid:\n",
        "    clf = SMOTE_XGB(XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42,\n",
        "                                  n_estimators=params['n_estimators'], max_depth=params['max_depth'], learning_rate=params['learning_rate']))\n",
        "    clf.fit(X_train_trans, y_train)\n",
        "    yhat_proba = clf.predict_proba(X_test_trans)[:,1]\n",
        "    ap = average_precision_score(y_test, yhat_proba)\n",
        "    print('params:', params, 'AP:', round(ap,4))\n",
        "    if ap > best_score:\n",
        "        best_score = ap\n",
        "        best_params = params\n",
        "        best_clf = clf\n",
        "print('\\nBest params by AP:', best_params, 'Best AP:', best_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i42yFlhBGct_",
        "outputId": "0b6fcdbb-8e23-4ca5-a109-a738e5afaac5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Confusion Matrix:\n",
            "[[1263767    7114]\n",
            " [      4    1639]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00   1270881\n",
            "           1       0.19      1.00      0.32      1643\n",
            "\n",
            "    accuracy                           0.99   1272524\n",
            "   macro avg       0.59      1.00      0.66   1272524\n",
            "weighted avg       1.00      0.99      1.00   1272524\n",
            "\n",
            "ROC-AUC: 0.9994\n",
            "PR-AUC (Average Precision): 0.7919\n",
            "\n",
            "Metrics saved to fraud_analysis_outputs/metrics.json\n"
          ]
        }
      ],
      "source": [
        "y_pred = best_clf.predict(X_test_trans)\n",
        "y_proba = best_clf.predict_proba(X_test_trans)[:,1]\n",
        "print('\\nConfusion Matrix:')\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print('\\nClassification Report:')\n",
        "print(classification_report(y_test, y_pred))\n",
        "print('ROC-AUC:', round(roc_auc_score(y_test, y_proba),4))\n",
        "print('PR-AUC (Average Precision):', round(average_precision_score(y_test, y_proba),4))\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "pr_auc = auc(recall, precision)\n",
        "metrics = {\n",
        "    'confusion_matrix': confusion_matrix(y_test, y_pred).tolist(),\n",
        "    'classification_report': classification_report(y_test, y_pred, output_dict=True),\n",
        "    'roc_auc': roc_auc_score(y_test, y_proba),\n",
        "    'pr_auc': average_precision_score(y_test, y_proba)\n",
        "}\n",
        "import json\n",
        "with open(OUTPUT_DIR / 'metrics.json', 'w') as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "print('\\nMetrics saved to', OUTPUT_DIR / 'metrics.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESE-3al8Grtc",
        "outputId": "f4b175f6-5983-41ab-b751-b4e6c3e3456b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top features:\n",
            "       feature  importance\n",
            "newbalanceOrig    0.508577\n",
            "  type_PAYMENT    0.206182\n",
            " type_TRANSFER    0.120683\n",
            " oldbalanceOrg    0.090435\n",
            "        amount    0.033906\n",
            " type_CASH_OUT    0.016612\n",
            "          step    0.011091\n",
            "  type_CASH_IN    0.005861\n",
            "    type_DEBIT    0.004264\n",
            "isFlaggedFraud    0.002388\n"
          ]
        }
      ],
      "source": [
        "num_feats = preprocessor.transformers_[0][2]\n",
        "cat_encoder = preprocessor.transformers_[1][1]\n",
        "cat_cols_ohe = []\n",
        "if low_card_cats:\n",
        "    cat_ohe = preprocessor.named_transformers_['cat']\n",
        "    try:\n",
        "        cat_feature_names = cat_ohe.get_feature_names_out(low_card_cats)\n",
        "    except Exception:\n",
        "        cat_feature_names = cat_ohe.get_feature_names(low_card_cats)\n",
        "    cat_cols_ohe = list(cat_feature_names)\n",
        "feature_names = list(num_feats) + cat_cols_ohe\n",
        "try:\n",
        "    importances = best_clf.estimator_.feature_importances_\n",
        "    fi = pd.DataFrame({'feature':feature_names, 'importance':importances})\n",
        "    fi = fi.sort_values('importance', ascending=False)\n",
        "    fi.to_csv(OUTPUT_DIR / 'feature_importance.csv', index=False)\n",
        "    print('\\nTop features:')\n",
        "    print(fi.head(20).to_string(index=False))\n",
        "except Exception as e:\n",
        "    print('Could not extract feature importances:', e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0_bFa7LG3Se",
        "outputId": "3065de29-2445-4ca1-eebb-07bf57a04782"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "SHAP top features:\n",
            "       feature  mean_abs_shap\n",
            " oldbalanceOrg       4.045572\n",
            "newbalanceOrig       3.590510\n",
            "  type_PAYMENT       1.823965\n",
            "        amount       1.530852\n",
            "          step       1.184622\n",
            "  type_CASH_IN       0.764720\n",
            " type_CASH_OUT       0.397444\n",
            " type_TRANSFER       0.262346\n",
            "    type_DEBIT       0.024236\n",
            "isFlaggedFraud       0.004996\n",
            "SHAP plot saved to fraud_analysis_outputs/shap_summary_plot.png\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    explainer = shap.TreeExplainer(best_clf.estimator_)\n",
        "    shap_values = explainer.shap_values(X_test_trans)\n",
        "    mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
        "    shap_df = pd.DataFrame({'feature':feature_names, 'mean_abs_shap':mean_abs_shap}).sort_values('mean_abs_shap', ascending=False)\n",
        "    shap_df.to_csv(OUTPUT_DIR / 'shap_summary.csv', index=False)\n",
        "    print('\\nSHAP top features:')\n",
        "    print(shap_df.head(20).to_string(index=False))\n",
        "    plt.figure(figsize=(8,6))\n",
        "    shap.summary_plot(shap_values, features=X_test_trans, feature_names=feature_names, show=False)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUTPUT_DIR / 'shap_summary_plot.png', dpi=150)\n",
        "    plt.close()\n",
        "    print('SHAP plot saved to', OUTPUT_DIR / 'shap_summary_plot.png')\n",
        "except Exception as e:\n",
        "    print('SHAP explanation failed:', e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVfMRWCbG93F",
        "outputId": "a85cb0d9-431c-4368-a8a6-6acc939de224"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Key factors and recommendations saved to fraud_analysis_outputs/key_factors_and_recommendations.txt\n",
            "\n",
            "All done. Check the fraud_analysis_outputs directory for artifacts: metrics.json, feature_importance.csv, shap_summary.csv, shap_summary_plot.png, key_factors_and_recommendations.txt\n"
          ]
        }
      ],
      "source": [
        "key_factors_text = []\n",
        "try:\n",
        "    top_features = fi.head(10)['feature'].tolist()\n",
        "    key_factors_text.append('Top features by model importance: ' + ', '.join(top_features))\n",
        "except:\n",
        "    key_factors_text.append('Top features could not be computed.')\n",
        "key_factors_text.append('\\nDo these factors make sense?\\n- Provide domain reasoning: e.g., high transaction amounts, unusual device/country, rapid frequency, mismatched billing/shipping info, and unusual merchant categories often indicate fraud. Use business context to validate features.')\n",
        "key_factors_text.append('\\nPrevention recommendations:\\n1. Rule-based blocking for extremely high-risk patterns.\\n2. Multi-factor authentication for high-value transactions.\\n3. Real-time scoring and manual review queue for medium-risk cases.\\n4. Regular model retraining and feature drift monitoring.\\n5. Logging and secure storage for audit trails.\\n6. Role-based access and secure data pipelines.\\n')\n",
        "key_factors_text.append('\\nHow to determine if these measures work:\\n- A/B test: route a % of traffic to the new infra and compare fraud rate, false positive rate, conversion rate.\\n- Monitor before/after metrics: fraud loss amount, detection rate, manual review workload, customer friction (drop-offs).\\n- Set statistical significance tests for changes in key metrics.\\n- Implement monitoring dashboards with alerting on drift in feature distributions and model performance metrics.\\n')\n",
        "with open(OUTPUT_DIR / 'key_factors_and_recommendations.txt', 'w') as f:\n",
        "    f.write('\\n\\n'.join(key_factors_text))\n",
        "print('\\nKey factors and recommendations saved to', OUTPUT_DIR / 'key_factors_and_recommendations.txt')\n",
        "print('\\nAll done. Check the fraud_analysis_outputs directory for artifacts: metrics.json, feature_importance.csv, shap_summary.csv, shap_summary_plot.png, key_factors_and_recommendations.txt')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
